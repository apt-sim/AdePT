// SPDX-FileCopyrightText: 2022 CERN
// SPDX-License-Identifier: Apache-2.0

#include <AdePT/core/AsyncAdePTTransport.hh>

#include <AdePT/integration/AdePTGeant4Integration.hh>

#include <VecGeom/base/Stopwatch.h>
#include <VecGeom/management/BVHManager.h>
#include <VecGeom/management/GeoManager.h>
#ifdef ADEPT_USE_SURF
#include <VecGeom/surfaces/BrepHelper.h>
#endif

#include <G4RunManager.hh>
#include <G4Threading.hh>
#include <G4Proton.hh>
#include <G4Region.hh>
#include <G4SDManager.hh>
#include <G4VFastSimSensitiveDetector.hh>
#include <G4MaterialCutsCouple.hh>
#include <G4ProductionCutsTable.hh>
#include <G4TransportationManager.hh>

#include <G4HepEmData.hh>
#include <G4HepEmState.hh>
#include <G4HepEmStateInit.hh>
#include <G4HepEmParameters.hh>
#include <G4HepEmMatCutData.hh>
#include <G4LogicalVolumeStore.hh>

#include <iomanip>

// std::shared_ptr<AdePTTransportInterface> AdePTTransportFactory(unsigned int nThread, unsigned int nTrackSlot,
//                                                                unsigned int nHitSlot, int verbosity,
//                                                                std::vector<std::string> const *GPURegionNames,
//                                                                bool trackInAllRegions, int cudaStackSize)
// {
//   static std::shared_ptr<AsyncAdePT::AsyncAdePTTransport> adePT{new AsyncAdePT::AsyncAdePTTransport(
//       nThread, nTrackSlot, nHitSlot, verbosity, GPURegionNames, trackInAllRegions, cudaStackSize)};
//   return adePT;
// }

/// Forward declarations
namespace async_adept_impl {
void CopySurfaceModelToGPU();
template <typename FieldType>
bool InitializeBField(FieldType &);
template <typename FieldType>
void FreeField();
G4HepEmState *InitG4HepEm(G4HepEmConfig *hepEmConfig);
void FlushScoring(AdePTScoring &);
std::shared_ptr<const std::vector<GPUHit>> GetGPUHits(unsigned int, AsyncAdePT::GPUstate &);
std::pair<GPUHit *, GPUHit *> GetGPUHitsFromBuffer(unsigned int, unsigned int, AsyncAdePT::GPUstate &, bool &);
void CloseGPUBuffer(unsigned int, AsyncAdePT::GPUstate &, GPUHit *, const bool);
std::thread LaunchGPUWorker(int, int, int, int, AsyncAdePT::TrackBuffer &, AsyncAdePT::GPUstate &,
                            std::vector<std::atomic<AsyncAdePT::EventState>> &, std::condition_variable &,
                            std::vector<AdePTScoring> &, int, int, bool, bool, unsigned short);
std::unique_ptr<AsyncAdePT::GPUstate, AsyncAdePT::GPUstateDeleter> InitializeGPU(
    int trackCapacity, int leakCapacity, int scoringCapacity, int numThreads, AsyncAdePT::TrackBuffer &trackBuffer,
    std::vector<AdePTScoring> &scoring, double CPUCapacityFactor, double CPUCopyFraction);
void FreeGPU(std::unique_ptr<AsyncAdePT::GPUstate, AsyncAdePT::GPUstateDeleter> &, G4HepEmState &, std::thread &);
} // namespace async_adept_impl

namespace AsyncAdePT {

namespace {
template <typename T>
std::size_t countTracks(int pdgToSelect, T const &container)
{
  return std::count_if(container.begin(), container.end(),
                       [pdgToSelect](TrackDataWithIDs const &track) { return track.pdg == pdgToSelect; });
}

std::ostream &operator<<(std::ostream &stream, TrackDataWithIDs const &track)
{
  const auto flags = stream.flags();
  stream << std::setw(5) << track.pdg << std::scientific << std::setw(15) << std::setprecision(6) << track.eKin << " ("
         << std::setprecision(2) << std::setw(9) << track.position[0] << std::setw(9) << track.position[1]
         << std::setw(9) << track.position[2] << ")";
  stream.flags(flags);
  return stream;
}
} // namespace

template <typename IntegrationLayer>
AsyncAdePTTransport<IntegrationLayer>::AsyncAdePTTransport(AdePTConfiguration &configuration,
                                                           G4HepEmTrackingManagerSpecialized *hepEmTM)
    : fAdePTSeed{configuration.GetAdePTSeed()}, fNThread{(ushort)configuration.GetNumThreads()},
      fTrackCapacity{(uint)(1024 * 1024 * configuration.GetMillionsOfTrackSlots())},
      fLeakCapacity{(uint)(1024 * 1024 * configuration.GetMillionsOfLeakSlots())},
      fScoringCapacity{(uint)(1024 * 1024 * configuration.GetMillionsOfHitSlots())},
      fDebugLevel{configuration.GetVerbosity()}, fCUDAStackLimit{configuration.GetCUDAStackLimit()},
      fCUDAHeapLimit{configuration.GetCUDAHeapLimit()}, fLastNParticlesOnCPU{configuration.GetLastNParticlesOnCPU()},
      fIntegrationLayerObjects(fNThread), fEventStates(fNThread), fGPUNetEnergy(fNThread, 0.0),
      fTrackInAllRegions{configuration.GetTrackInAllRegions()}, fGPURegionNames{configuration.GetGPURegionNames()},
      fCPURegionNames{configuration.GetCPURegionNames()}, fReturnAllSteps{configuration.GetCallUserSteppingAction()},
      fReturnFirstAndLastStep{configuration.GetCallUserTrackingAction() || configuration.GetCallUserSteppingAction()},
      fBfieldFile{configuration.GetCovfieBfieldFile()}, fCPUCapacityFactor{configuration.GetCPUCapacityFactor()},
      fCPUCopyFraction{configuration.GetHitBufferFlushThreshold()}
{
  if (fNThread > kMaxThreads)
    throw std::invalid_argument("AsyncAdePTTransport limited to " + std::to_string(kMaxThreads) + " threads");

  for (auto &eventState : fEventStates) {
    std::atomic_init(&eventState, EventState::ScoringRetrieved);
  }

  AsyncAdePTTransport::Initialize(hepEmTM);
}

template <typename IntegrationLayer>
AsyncAdePTTransport<IntegrationLayer>::~AsyncAdePTTransport()
{
  async_adept_impl::FreeGPU(std::ref(fGPUstate), *fg4hepem_state, fGPUWorker);
}

template <typename IntegrationLayer>
void AsyncAdePTTransport<IntegrationLayer>::SetHepEmTrackingManagerForThread(int threadId,
                                                                             G4HepEmTrackingManagerSpecialized *hepEmTM)
{
  fIntegrationLayerObjects[threadId].SetHepEmTrackingManager(hepEmTM);
}

template <typename IntegrationLayer>
void AsyncAdePTTransport<IntegrationLayer>::AddTrack(
    int pdg, uint64_t trackId, uint64_t parentId, double energy, double x, double y, double z, double dirx, double diry,
    double dirz, double globalTime, double localTime, double properTime, float weight, unsigned short stepCounter,
    int threadId, unsigned int eventId, vecgeom::NavigationState &&state, vecgeom::NavigationState &&originState)
{
  if (pdg != 11 && pdg != -11 && pdg != 22) {
    G4cerr << __FILE__ << ":" << __LINE__ << ": Only supporting EM tracks. Got pdgID=" << pdg << "\n";
    return;
  }

  TrackDataWithIDs track{pdg,
                         trackId,
                         parentId,
                         energy,
                         x,
                         y,
                         z,
                         dirx,
                         diry,
                         dirz,
                         globalTime,
                         localTime,
                         properTime,
                         weight,
                         stepCounter,
                         std::move(state),
                         std::move(originState),
                         eventId,
                         static_cast<short>(threadId)};
  if (fDebugLevel >= 2) {
    fGPUNetEnergy[threadId] += energy;
    if (fDebugLevel >= 6) {
      G4cout << "\n[_in," << eventId << "," << trackId << "]: " << track << "\tGPU net energy " << std::setprecision(6)
             << fGPUNetEnergy[threadId] << G4endl;
    }
  }

  // Lock buffer and emplace the track
  {
    auto trackHandle  = fBuffer->createToDeviceSlot();
    trackHandle.track = std::move(track);
  }

  fEventStates[threadId].store(EventState::NewTracksFromG4, std::memory_order_release);
}

template <typename IntegrationLayer>
bool AsyncAdePTTransport<IntegrationLayer>::InitializeBField()
{
  if (!fBfieldFile.empty()) {
    // Initialize magnetic field data from file
    if (!fMagneticField.InitializeFromFile(fBfieldFile)) {
      return false;
    }

    // Delegate the setup of the global view pointer to adept_impl
    return async_adept_impl::InitializeBField(fMagneticField);
  }
  return true; // no file provided, but no error encountered, so true is returned
}

template <typename IntegrationLayer>
bool AsyncAdePTTransport<IntegrationLayer>::InitializeBField(UniformMagneticField &Bfield)
{
  vecgeom::Vector3D<float> position{
      0.,
      0.,
      0.,
  };
  auto field_value = Bfield.Evaluate(position);
  if (field_value.Mag2() > 0) {
    // Delegate the setup of the global view pointer to adept_impl
    return async_adept_impl::InitializeBField(Bfield);
  }
  return true; // no B field provided, but no error encountered, so true is returned
}

template <typename IntegrationLayer>
bool AsyncAdePTTransport<IntegrationLayer>::InitializeGeometry(const vecgeom::cxx::VPlacedVolume *world)
{
  // Upload geometry to GPU.
  auto &cudaManager = vecgeom::cxx::CudaManager::Instance();
  if (fCUDAStackLimit > 0) {
    std::cout << "CUDA Device stack limit: " << fCUDAStackLimit << "\n";
    cudaDeviceSetLimit(cudaLimitStackSize, fCUDAStackLimit);
  }
  if (fCUDAHeapLimit > 0) {
    std::cout << "CUDA Device heap limit: " << fCUDAHeapLimit << "\n";
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, fCUDAHeapLimit);
  }
  bool success = true;
#ifdef ADEPT_USE_SURF
#ifdef ADEPT_MIXED_PRECISION
  using SurfData   = vgbrep::SurfData<float>;
  using BrepHelper = vgbrep::BrepHelper<float>;
#else
  using SurfData   = vgbrep::SurfData<double>;
  using BrepHelper = vgbrep::BrepHelper<double>;
#endif
  vecgeom::Stopwatch timer;
  timer.Start();
  if (!BrepHelper::Instance().Convert()) return 1;
  BrepHelper::Instance().PrintSurfData();
  std::cout << "== Conversion to surface model done in " << timer.Stop() << " [s]\n";
  // Upload only navigation table to the GPU
  cudaManager.SynchronizeNavigationTable();
  async_adept_impl::CopySurfaceModelToGPU();
#else
  // Upload solid geometry to GPU.
  cudaManager.LoadGeometry(world);
  auto world_dev = cudaManager.Synchronize();
  success        = world_dev != nullptr;
  // Initialize BVH
  InitBVH();
#endif
  return success;
}

template <typename IntegrationLayer>
bool AsyncAdePTTransport<IntegrationLayer>::InitializePhysics(G4HepEmConfig *hepEmConfig)
{
  // Initialize shared physics data
  fg4hepem_state.reset(async_adept_impl::InitG4HepEm(hepEmConfig));
  return true;
}

template <typename IntegrationLayer>
void AsyncAdePTTransport<IntegrationLayer>::Initialize(G4HepEmTrackingManagerSpecialized *hepEmTM)
{
  const auto numVolumes = vecgeom::GeoManager::Instance().GetRegisteredVolumesCount();
  if (numVolumes == 0) throw std::runtime_error("AsyncAdePTTransport::Initialize: Number of geometry volumes is zero.");

  G4cout << "=== AsyncAdePTTransport: initializing geometry and physics\n";
  // Initialize geometry on device
  if (!vecgeom::GeoManager::Instance().IsClosed())
    throw std::runtime_error("AsyncAdePTTransport::Initialize: VecGeom geometry not closed.");

  const vecgeom::cxx::VPlacedVolume *world = vecgeom::GeoManager::Instance().GetWorld();
  if (!InitializeGeometry(world))
    throw std::runtime_error("AsyncAdePTTransport::Initialize: Cannot initialize geometry on GPU");

  // Initialize G4HepEm
  if (!InitializePhysics(hepEmTM->GetConfig()))
    throw std::runtime_error("AsyncAdePTTransport::Initialize cannot initialize physics on GPU");

  // Initialize field
#ifdef ADEPT_USE_EXT_BFIELD
  // Try to initialize field from file
  if (!InitializeBField()) {
    throw std::runtime_error(
        "AdePTTransport<IntegrationLayer>::Initialize cannot initialize GeneralMagneticField on GPU");
  }
#else
  auto field_values =
      fIntegrationLayerObjects.front().GetUniformField(); // Get the field value from one of the worker threads
  std::unique_ptr<UniformMagneticField> Bfield = std::make_unique<UniformMagneticField>(field_values);
  if (!InitializeBField(*Bfield))
    throw std::runtime_error("AdePTTransport<IntegrationLayer>::Initialize cannot initialize field on GPU");
#endif

  // Check VecGeom geometry matches Geant4. Initialize auxiliary per-LV data. Initialize scoring map.
  fIntegrationLayerObjects.front().CheckGeometry(fg4hepem_state.get());
  adeptint::VolAuxData *auxData = new adeptint::VolAuxData[vecgeom::GeoManager::Instance().GetRegisteredVolumesCount()];
  adeptint::WDTHostRaw wdtRaw;
  fIntegrationLayerObjects.front().InitVolAuxData(auxData, fg4hepem_state.get(), hepEmTM, fTrackInAllRegions,
                                                  fGPURegionNames, wdtRaw);

  // Initialize volume auxiliary data on device
  auto &volAuxArray       = adeptint::VolAuxArray::GetInstance();
  volAuxArray.fNumVolumes = numVolumes;
  volAuxArray.fAuxData    = auxData;
  AsyncAdePT::InitVolAuxArray(volAuxArray);

  // Pack host-side WDT
  adeptint::WDTHostPacked wdtPacked = adeptint::PackWDT(wdtRaw);

  // Upload WDT to device
  adeptint::WDTDeviceBuffers wdtDev;
  adeptint::WDTDeviceView wdtViewHostCopy;
  AsyncAdePT::InitWDTOnDevice(wdtPacked, wdtDev, wdtViewHostCopy);

  // FIXME freeing is missing
  // (Store wdtDev in a member of your transport so you can FreeWDTOnDevice(...) in dtor/cleanup)
  // this->fWDTDevBuffers = wdtDev; // e.g. add a member in transport class

  // FIXME: The size of the buffer should be more explicitly configured
  // NOTE: Increasing the size of the leak buffer size can probably improve performance
  // Allocate buffers to transport particles to/from device. Scale the size of the staging area
  // with the number of threads.
  G4cout << "\nAllocating " << 4 * 8192 * fNThread << " To-device buffer slots" << G4endl;
  G4cout << "\nAllocating " << 2048 * fNThread << " From-device buffer slots" << G4endl;
  fBuffer = std::make_unique<TrackBuffer>(4 * 8192 * fNThread, 2048 * fNThread, fNThread);

  assert(fBuffer != nullptr);

  fGPUstate  = async_adept_impl::InitializeGPU(fTrackCapacity, fLeakCapacity, fScoringCapacity, fNThread, *fBuffer,
                                               fScoring, fCPUCapacityFactor, fCPUCopyFraction);
  fGPUWorker = async_adept_impl::LaunchGPUWorker(
      fTrackCapacity, fLeakCapacity, fScoringCapacity, fNThread, *fBuffer, *fGPUstate, fEventStates, fCV_G4Workers,
      fScoring, fAdePTSeed, fDebugLevel, fReturnAllSteps, fReturnFirstAndLastStep, fLastNParticlesOnCPU);
}

template <typename IntegrationLayer>
void AsyncAdePTTransport<IntegrationLayer>::InitBVH()
{
  vecgeom::cxx::BVHManager::Init();
  vecgeom::cxx::BVHManager::DeviceInit();
}

template <typename IntegrationLayer>
void AsyncAdePTTransport<IntegrationLayer>::ProcessGPUSteps(int threadId, int eventId)
{

  AdePTGeant4Integration &integrationInstance = fIntegrationLayerObjects[threadId];
  std::pair<GPUHit *, GPUHit *> range;
  bool dataOnBuffer;

  while ((range = async_adept_impl::GetGPUHitsFromBuffer(threadId, eventId, *fGPUstate, dataOnBuffer)).first !=
         nullptr) {
    for (auto it = range.first; it != range.second; ++it) {
      // important sanity check: thread should only process its own hits and only from the current event
      if (it->threadId != threadId)
        std::cerr << "\033[1;31mError, threadId doesn't match it->threadId " << it->threadId << " threadId " << threadId
                  << "\033[0m" << std::endl;
      if (it->fEventId != eventId) {
        std::cerr << "\033[1;31mError, eventId doesn't match it->fEventId " << it->fEventId << " eventId " << eventId
                  << " num hits to be processed " << (range.second - range.first) << " dataOnBuffer " << dataOnBuffer
                  << " state : " << static_cast<unsigned int>(fEventStates[threadId].load(std::memory_order_acquire))
                  << " trackID " << it->fTrackID << " parentID " << it->fParentID << " StepNumber " << it->fStepCounter
                  << " ptype " << static_cast<short>(it->fParticleType) << " stepLimit / creator process "
                  << it->fStepLimProcessId << "\033[0m" << std::endl;
      }
      integrationInstance.ProcessGPUStep(*it, fReturnAllSteps, fReturnFirstAndLastStep);
    }
    async_adept_impl::CloseGPUBuffer(threadId, *fGPUstate, range.first, dataOnBuffer);
  }
}

template <typename IntegrationLayer>
void AsyncAdePTTransport<IntegrationLayer>::Flush(G4int threadId, G4int eventId)
{
  if (fDebugLevel >= 3) {
    G4cout << "\nFlushing AdePT for event " << eventId << G4endl;
  }

  assert(static_cast<unsigned int>(threadId) < fBuffer->fromDeviceBuffers.size());
  fEventStates[threadId].store(EventState::G4RequestsFlush, std::memory_order_release);

  AdePTGeant4Integration &integrationInstance = fIntegrationLayerObjects[threadId];

  while (fEventStates[threadId].load(std::memory_order_acquire) < EventState::DeviceFlushed) {

    {
      std::unique_lock lock{fMutex_G4Workers};
      fCV_G4Workers.wait(lock);
    }

    ProcessGPUSteps(threadId, eventId);
  }

  // Now device should be flushed, so retrieve the tracks:
  std::vector<TrackDataWithIDs> tracks;
  {
    auto handle = fBuffer->getTracksFromDevice(threadId);
    tracks.swap(handle.tracks);
    fEventStates[threadId].store(EventState::LeakedTracksRetrieved, std::memory_order_release);
  }

  // TODO: Sort tracks on device?
#ifndef NDEBUG
  for (auto const &track : tracks) {
    bool error = false;
    if (track.threadId != threadId || track.eventId != static_cast<unsigned int>(eventId)) error = true;
    if (!(track.pdg == -11 || track.pdg == 11 || track.pdg == 22)) error = true;
    if (error)
      std::cerr << "Error in returning track: threadId=" << track.threadId << " eventId=" << track.eventId
                << " pdg=" << track.pdg << "\n";
    assert(!error);
  }
#endif

  // Sort the tracks coming from device by energy. This is necessary to ensure reproducibility
  std::sort(tracks.begin(), tracks.end());

  const auto oldEnergyTransferred = fGPUNetEnergy[threadId];
  if (fDebugLevel >= 2) {
    unsigned int trackId = 0;
    for (const auto &track : tracks) {

      fGPUNetEnergy[threadId] -= track.eKin;
      if (fDebugLevel >= 5) {
        G4cout << "\n[out," << track.eventId << "," << trackId++ << "]: " << track << "\tGPU net energy "
               << std::setprecision(6) << fGPUNetEnergy[threadId] << G4endl;
      }
    }
  }

  if (fDebugLevel >= 2) {
    std::stringstream str;
    str << "\n[" << eventId << "]: Pushed " << tracks.size() << " tracks to G4";
    str << "\tEnergy back to G4: " << std::setprecision(6)
        << (oldEnergyTransferred - fGPUNetEnergy[threadId]) / CLHEP::GeV << "\tGPU net energy " << std::setprecision(6)
        << fGPUNetEnergy[threadId] / CLHEP::GeV << " GeV";
    str << "\t(" << countTracks(11, tracks) << ", " << countTracks(-11, tracks) << ", " << countTracks(22, tracks)
        << ")";
    G4cout << str.str() << G4endl;
  }

  integrationInstance.ReturnTracks(tracks.begin(), tracks.end(), fDebugLevel, fReturnFirstAndLastStep);

  async_adept_impl::FlushScoring(fScoring[threadId]);
  fEventStates[threadId].store(EventState::ScoringRetrieved, std::memory_order_release);
}

} // namespace AsyncAdePT
